{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Znovupou≈æitie - Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "\n",
    "import category_encoders as ce\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats as sm_stats\n",
    "import statsmodels.stats.api as sms\n",
    "\n",
    "import vizualizacia_funkcie as visual\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn import impute \n",
    "from sklearn import preprocessing\n",
    "from sklearn import pipeline\n",
    "from sklearn import base\n",
    "from sklearn import compose\n",
    "from sklearn import feature_selection\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "\n",
    "import imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"./data/personal_train.csv\", index_col=0)\n",
    "df2 = pd.read_csv(\"./data/other_train.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tu nizsie mame funkcie, ktore boli deklarovane a pouzite v preprocessing etape. Dovodom, preco kopirujeme vsetky tieto funkcie do tohto notebooku je kvoli prehladnosti, ako aj kvoli tomu, ze potom dany notebook upravime na script, ktory nasledne pouzijeme v dalsej etape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining a merging dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funckia, ktora mergne zaznamy, ktore su rovnake\n",
    "def piece_datarows_together(data):\n",
    "    \n",
    "    data = data.copy().set_index(\"name\")\n",
    "    \n",
    "    #toto nam vrati dataset, ktory obsahuje vsetky duplikaty, s ktorymi budeme pracovat\n",
    "    #proste to vrati data, ktore maju index, ktory je v datasete viac ako raz pouzity\n",
    "    duplicated = data[data.index.duplicated(keep=False)]\n",
    "    \n",
    "    index_values = duplicated.index.unique()\n",
    "    \n",
    "    #najprv vsetky hodnoty prenesieme do prveho vyskytu zaznamu daneho pacienta v datasete\n",
    "    for idx in index_values:\n",
    "        mini_dataset = duplicated.loc[idx] #toto vrati viacero zaznamov s rovnakych idx\n",
    "        \n",
    "        #zistim si, ktore atributy su nullove pre presne prvy zaznam a pre konkretne nullove atributy budem nadalej hladat\n",
    "        #nenullovu hodnotu v ostatnych zaznamoch s rovnakym idx\n",
    "        missing_mask = mini_dataset.iloc[0].isnull()\n",
    "        attributes = mini_dataset.columns.values\n",
    "        missing_attributes = attributes[missing_mask]\n",
    "        \n",
    "        #tu replacujem null hodnoty za nenullove\n",
    "        for attr in missing_attributes:\n",
    "            not_null = mini_dataset[attr][mini_dataset[attr].notnull()]\n",
    "            \n",
    "            if len(not_null) != 0:\n",
    "                mini_dataset.iloc[0][attr] = not_null.values[0]\n",
    "        \n",
    "        \n",
    "    #teraz uz mozme vymazat vsetky druhe, resp. ostatne zaznamy pacienta\n",
    "    duplicated_mask = data.index.duplicated(keep=\"first\")\n",
    "    \n",
    "    data = data.reset_index()\n",
    "    duplicated_indices = data.index.values[duplicated_mask]\n",
    "    \n",
    "    \n",
    "    return data.drop(index=duplicated_indices).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funkcia, ktora joine obi dva dataframy, s ktorymi pracujeme + mergne riadky, kde su splittnute data\n",
    "def one_proper_df(df1, df2, return_X_y=True):\n",
    "    data = df1.drop(columns=[\"address\"]).set_index(\"name\").join(df2.set_index(\"name\"), how=\"right\").reset_index()\n",
    "    data = piece_datarows_together(data)\n",
    "    \n",
    "    if return_X_y == True:\n",
    "        X = data.drop(columns=[\"class\"])\n",
    "        y = data[\"class\"]\n",
    "        return X,y\n",
    "    \n",
    "    else:\n",
    "        return data \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tu nizsie mame funkcie, ktore pouzivame na zmensenie poctu hodnot kategorickych atributov. Vyber atributov, ktore sa merguju, sme vybrali este pocas fazy analyzy, kedy malo pocetne hodnoty su mergnute do jednej hodnoty, aby hodnoty daneho atributu boli viac vyrovnane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prvotne preprocessing kroky - cez FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marital_status_categories(row):\n",
    "    \n",
    "    ms = row[\"marital-status\"]\n",
    "        \n",
    "    if ms is not np.nan and ms not in (\"Divorced\", \"Never-married\", \"Married-civ-spouse\"):\n",
    "        row[\"marital-status\"] = \"Other\"\n",
    "        \n",
    "    return row\n",
    "\n",
    "def relationship_categories(row):\n",
    "    \n",
    "    rel = row[\"relationship\"]\n",
    "        \n",
    "    if rel is not np.nan and rel not in (\"Not-in-family\", \"Husband\", \"Own-child\"):\n",
    "        row[\"relationship\"] = \"Other\"\n",
    "        \n",
    "    return row\n",
    "\n",
    "def occupation_categories(row):\n",
    "\n",
    "    occ = row[\"occupation\"]\n",
    "    \n",
    "    if occ is not np.nan and occ not in (\"Craft-repair\", \"Prof-specialty\", \"Exec-managerial\", \n",
    "                                         \"Adm-clerical\", \"Sales\", \"Other-service\", \"Machine-op-inspct\", \n",
    "                                         \"Transport-moving\"):\n",
    "        \n",
    "        row[\"occupation\"] = \"Other\"\n",
    "        \n",
    "    return row\n",
    "\n",
    "def workclass_categories(row):\n",
    "\n",
    "    wc = row[\"workclass\"]\n",
    "    \n",
    "    if wc is not np.nan and wc != \"Private\":\n",
    "        row[\"workclass\"] = \"Non-private\"\n",
    "        \n",
    "    return row\n",
    "\n",
    "#oproti ostatnym funkciam v tejto bunke, tato funkcia sluzi na transformaciu spojiteho atributu hours-per-week na kategoricky\n",
    "def categorize_hours(row):\n",
    "    \n",
    "    hour = row[\"hours-per-week\"]\n",
    "    \n",
    "    if math.isnan(hour):\n",
    "        row[\"hours-per-week-cat\"] = math.nan\n",
    "    elif hour <= 35:\n",
    "        row[\"hours-per-week-cat\"] = \"<=35\"\n",
    "    elif hour <= 45:\n",
    "        row[\"hours-per-week-cat\"] = \"35< hours <=45\"\n",
    "    elif hour > 45:\n",
    "        row[\"hours-per-week-cat\"] = \">45\"        \n",
    "\n",
    "    return row\n",
    "\n",
    "def simplify_education(row):\n",
    "        \n",
    "    edu = row[\"education\"]\n",
    "        \n",
    "    if edu is np.nan:\n",
    "        row[\"simple-edu\"] = edu\n",
    "        \n",
    "    elif re.match(\"^([0-9][a-zA-Z])|(1[0-2][a-zA-Z])\", edu) or edu == \"Preschool\":\n",
    "        row[\"simple-edu\"] = \"Attending-school\"\n",
    "        \n",
    "    elif edu in [\"Assoc-acdm\", \"Assoc-voc\", \"Prof-school\"]:\n",
    "        row[\"simple-edu\"] = \"Edu after HS, no uni\"\n",
    "        \n",
    "    elif edu in [\"Masters\", \"Doctorate\"]:\n",
    "        row[\"simple-edu\"] = \"Masters/Doctorate\"\n",
    "        \n",
    "    else:\n",
    "        row[\"simple-edu\"] = row[\"education\"]\n",
    "    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tu su nejake cary-mary, kedy s atributu date_of_birth, chceme ziskat rok narodenia, ktory nasledne mozeme pouzit na imputaciu missing values, ci zlych hodnot atributu age - totiz age ma v sebe zle namerane hodnoty, ktore su bud zaporne, alebo velmi velke (v tisickach), a tak dane zle hodnoty rovno nastavime na np.nan, pricom ich nasledne imputujeme pomocou roku narodenia, co, ako som uz napisal, ziskavame pomocou tejto funkcie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_formatting(data):    \n",
    "    \n",
    "    data = data.copy()\n",
    "    \n",
    "    import re\n",
    "    dates = []\n",
    "\n",
    "    for index,row in data.iterrows():\n",
    "        dates.append(re.sub('\\d', '*',  row['date_of_birth']))\n",
    "\n",
    "    dates = list(set(dates))\n",
    "    dates\n",
    "\n",
    "    from datetime import datetime\n",
    "\n",
    "    for index,row in data.iterrows():\n",
    "        line = row['date_of_birth']\n",
    "        if re.match(r\"^\\d{2}-\\d{2}-\\d{2}$\", line):\n",
    "            regex1 = line[0:2]\n",
    "            regex2 = line[3:5]\n",
    "            regex3 = line[6:8]\n",
    "\n",
    "            verbose = False\n",
    "            if (verbose == True):\n",
    "                if (int(regex1) > 31):\n",
    "                    print('Prvy udaj > 31: ',regex1)\n",
    "                if (int(regex2) > 31):\n",
    "                    print('Druhy udaj > 31: ',regex2)\n",
    "                if (int(regex3) > 31):\n",
    "                    print('Treti udaj > 31: ',regex3)\n",
    "\n",
    "    data['date_of_birth'] = data['date_of_birth'].map(lambda x: x[:10])\n",
    "    \n",
    "    for index,row in data.iterrows():\n",
    "        line = row['date_of_birth']\n",
    "        dateObj = None\n",
    "        if re.match(r\"^\\d{2}-\", line):\n",
    "            newDate = '19' + line\n",
    "            dateObj = datetime.strptime(newDate,'%Y-%m-%d')\n",
    "        elif re.match(r\"^\\d{4}-\", line):\n",
    "            dateObj = datetime.strptime(line,'%Y-%m-%d')\n",
    "        elif re.match(r\"^\\d{4}/\", line):\n",
    "            dateObj = datetime.strptime(line,'%Y/%m/%d')\n",
    "        elif re.match(r\"^\\d{2}/\", line):\n",
    "            dateObj = datetime.strptime(line,'%d/%m/%Y')\n",
    "        data.at[index,'date_of_birth'] = dateObj.strftime('%d-%m-%Y')\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tu nizsie mame rozne funckie, ktore aplikujeme v prvotnej faze pipelinu, kedy pouzivame triedu preprocessing.FunctionTransformer, ktory dovoluje aplikovanie custom funkcie na nas dataset. Teda pocas tejto prvotnej fazy aplikujeme na dataset jednoduche operacie, ktore opravuju nejake atributy, ako napriklad odstranovanie white spacov, ci ziskanie novych atributov z atributu medical_info, alebo odstranovanie useless atributov, vid nizsie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_useless_features(X):\n",
    "    \n",
    "    X = X.copy()\n",
    "    \n",
    "    useless_cols = [\"name\", \"race\", \"pregnant\", \"capital-loss\", \"capital-gain\", \"fnlwgt\", \"native-country\", \"address\"]\n",
    "    \n",
    "    return X.drop(columns=useless_cols)\n",
    "\n",
    "def add_oxygen_features(X):\n",
    "    X = X.copy()\n",
    "    \n",
    "    X[\"mean_oxygen\"] = 0\n",
    "    X[\"std_oxygen\"] = 0\n",
    "    X[\"kurtosis_oxygen\"] = 0\n",
    "    X[\"skewness_oxygen\"] = 0\n",
    "    \n",
    "    X = X.apply(get_oxygen_stats, axis=1)\n",
    "    \n",
    "    return X.drop(columns=[\"medical_info\"])\n",
    "\n",
    "#ziskavam 4 atributy o kysliku z atributu medical_info\n",
    "def get_oxygen_stats(row):\n",
    "    \n",
    "    string = row[\"medical_info\"]\n",
    "    \n",
    "    if string is np.nan:\n",
    "        return row\n",
    "    \n",
    "    string = string.replace(\"\\'\", \"\\\"\")\n",
    "    di = json.loads(string)\n",
    "    \n",
    "    for k in di.keys():\n",
    "        row[k] = float(di[k])\n",
    "        \n",
    "    return row\n",
    "\n",
    "def string_wrap_formatting(X):\n",
    "    X = X.copy()\n",
    "    return X.apply(string_formatting, axis=0)\n",
    "\n",
    "#vymazem white spacy a \"?\" vymenim za np.nan pre vsetky atributy typu \"O\" - object - string\n",
    "def string_formatting(col):\n",
    "    \n",
    "    if col.dtype == \"O\":\n",
    "        col = col.apply(lambda row: row.strip() if row is not np.nan else row)\n",
    "        col = col.apply(lambda row: np.nan if row is not np.nan and row == \"?\" else row)\n",
    "    \n",
    "    return col\n",
    "\n",
    "#tato funkcia je wrapper, ktory aplikuje funkcie na zmensenie poctu hodnot kategorickych atributov\n",
    "def bucket_cat_attr(X):\n",
    "   \n",
    "    X = X.copy()\n",
    "    \n",
    "    X = X.apply(marital_status_categories, axis=1)\n",
    "    X = X.apply(relationship_categories, axis=1)\n",
    "    X = X.apply(occupation_categories, axis=1)\n",
    "    X = X.apply(workclass_categories, axis=1)\n",
    "    \n",
    "    X[\"hours-per-week-cat\"] = 0\n",
    "    X = X.apply(categorize_hours, axis=1)\n",
    "    X = X.drop(columns=[\"hours-per-week\"])\n",
    "    \n",
    "    return X\n",
    "\n",
    "#atribut mean_glucose, je potrebne pretypovat\n",
    "def repair_mean_glucose(X):\n",
    "    \n",
    "    X = X.copy()\n",
    "    X[\"mean_glucose\"] = pd.to_numeric(X['mean_glucose'], errors= 'coerce')\n",
    "    return X\n",
    "\n",
    "#tu najprv extraktujeme rok narodenia z atributu date_of_birth, nasledne nullujeme zle hodnoty v atribute age, a rovno aj\n",
    "#imputujeme hodnoty v atribute age pomocou extrahovanych rokou narodenia\n",
    "def prepare_age(X):\n",
    "    X = X.copy()\n",
    "    X = date_formatting(X)\n",
    "    \n",
    "    X = X.apply(make_bs_age_nan, axis=1)\n",
    "    X = X.apply(calculate_age, axis=1)\n",
    "    \n",
    "    X = X.drop(columns=[\"date_of_birth\"])\n",
    "    \n",
    "    return X\n",
    "    \n",
    "#zle hodnoty agu -> np.nan\n",
    "def make_bs_age_nan(row):\n",
    "    \n",
    "    age = row[\"age\"]\n",
    "    \n",
    "    if age is np.nan:\n",
    "        return row\n",
    "    \n",
    "    if age <= 0 or age >= 100:\n",
    "        row[\"age\"] = np.nan\n",
    "        \n",
    "    return row\n",
    "\n",
    "#imputovanie hodnot agu pomocou roku narodenia\n",
    "def calculate_age(row):\n",
    "    \n",
    "    if row[\"age\"] is np.nan or math.isnan(row[\"age\"]):\n",
    "    \n",
    "        born = row[\"date_of_birth\"]\n",
    "\n",
    "        born = datetime.strptime(born, \"%d-%m-%Y\").date()\n",
    "        today = date.today()\n",
    "        \n",
    "        row[\"age\"] = today.year - born.year - ((today.month, today.day) < (born.month, born.day))\n",
    " \n",
    "        \n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takto vyzera prvy krok pipelinu. Chcel som prvotne vnorit tento pipeline do hlavneho pipelinu, ale asi imblearn pipeline to neumoznuje, lebo sa mu to nepacilo, takze tieto kroky su rozpisane v hlavnom pipeline. Avsak tu je ten pipeline zobrazeny len pre vizualizacne ucely.\n",
    "\n",
    "Dovodom, preco pouzivame imblearn je aplikacia resamplingu - konkretne odstranovanie outlierov pocas pipelinu. To klasicky scikit pipeline nedokaze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('feature_removal',\n",
       "                 FunctionTransformer(func=<function remove_useless_features at 0x000001BCADB245E0>)),\n",
       "                ('add_oxygen_attr',\n",
       "                 FunctionTransformer(func=<function add_oxygen_features at 0x000001BCADB24670>)),\n",
       "                ('mean_glucose_to_num',\n",
       "                 FunctionTransformer(func=<function repair_mean_glucose at 0x000001BCADB24AF0>)),\n",
       "                ('string_formatting',\n",
       "                 FunctionTransformer(func=<function string_wrap_formatting at 0x000001BCADB248B0>)),\n",
       "                ('bucket_cat_attr',\n",
       "                 FunctionTransformer(func=<function bucket_cat_attr at 0x000001BCADB24A60>))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.Pipeline(steps=[\n",
    "    (\"feature_removal\", preprocessing.FunctionTransformer(remove_useless_features)),\n",
    "    (\"add_oxygen_attr\", preprocessing.FunctionTransformer(add_oxygen_features)),\n",
    "    (\"mean_glucose_to_num\", preprocessing.FunctionTransformer(repair_mean_glucose)),\n",
    "    (\"string_formatting\", preprocessing.FunctionTransformer(string_wrap_formatting)),\n",
    "    (\"bucket_cat_attr\", preprocessing.FunctionTransformer(bucket_cat_attr))\n",
    "])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tato funkcia je ekvivalentna s hociktorym inym transformatorom v scikit-learne, ci uz na imputaciu, transformaciu, scaling a ine veci\n",
    "#jediny rozdiel je, ze to nevrati numpy array, ale DataFrame, a vdaka tomu si uchvoavam nazvy stlpcov\n",
    "#toto je klucove, pokial chcem pouzivat napriklad ColumnTransformer v dalsom kroku pipelinu, pokial chcem referovat jednotlive atributy\n",
    "#dataframu na zaklade ich mena\n",
    "class KeepDataFrame(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def __init__(self, transformation):\n",
    "        self.transformation = transformation\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \n",
    "        if self.transformation is not None:\n",
    "            self.transformation.fit(X)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \n",
    "        if self.transformation is not None:\n",
    "        \n",
    "            X = X.copy()\n",
    "            cols = X.columns\n",
    "            indices = X.index\n",
    "\n",
    "            X = self.transformation.transform(X)\n",
    "\n",
    "            X = pd.DataFrame(X, columns=cols, index=indices)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Toto je custom transformator, ktory sluzi na imputaciu kategorickych atributov prostrednictvom bud\n",
    "#knn imputera alebo iterative imputera\n",
    "#Najprv sa kategoricky atribut pretypuje na ciselny pomocou ce.OrdinalEncoder, nasledne dojde k imputacii\n",
    "#a potom sa znova inverznou funkciou vrati do kategorickeho atributu.\n",
    "#v tejto faze, kedy to uz pouzivame v pipeline, to nie je az take klucove transformovat naspat do kat. atributu\n",
    "#no vyuzili sme funkciu, ktoru sme predtym vytvorili, kedze sme nasledne imputovany atribut znova analyzovali,\n",
    "#co bol aj dovod, preco sme pouzili nan inverznu transformaciu\n",
    "class CustomCatImputing(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def __init__(self, imputer_type=\"knn\"):\n",
    "        self.ordinal_encoder = None\n",
    "        self.imputer = None\n",
    "        self.imputer_type = imputer_type\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \n",
    "        X = X.copy()\n",
    "        \n",
    "        columns = X.columns.values\n",
    "        indices = X.index\n",
    "\n",
    "        #toto sme uz riesili v preprocessing notebooku - chceme, aby nam null hodnoty neinkrementovali encoding hodnoty v strede datasetu,\n",
    "        #ale aby sme mali urcity range celociselnych hodnot, bez dier, ktore sa pouzije v imputerovi\n",
    "        #je to klucove aj pri KNN imputerovi, aj pri Iterative imputerovi, lebo pri iterative pracujeme so ciselnymi hodnotami,\n",
    "        #ktore su kludne aj desatinne, a teda nakoniec sa vysledok imputera rounduje\n",
    "        #a pri knn sice pracujeme s celocislenymi cislami, no nakoniec imputuje sa priemer ziskany z danych\n",
    "        #n-susedov, co znova moze byt desatinne cislo\n",
    "        #takze, aby sme nahodou pri roundovani sa nedostali na encoding hodnotu, ktora patri null hodnote, tak \n",
    "        #feedujeme danemu ordinal encodingu hned na zaciatku null hodnoty\n",
    "        null_values = pd.DataFrame(index=pd.Index([-1]), columns=columns, data=[[np.nan for i in range(len(columns))]])\n",
    "        X = pd.concat([null_values,X])\n",
    "\n",
    "        self.ordinal_encoder = ce.ordinal.OrdinalEncoder(handle_missing=\"return_nan\", handle_unknown=\"return_nan\")\n",
    "        X = self.ordinal_encoder.fit_transform(X)\n",
    "        \n",
    "        X = X[1:]\n",
    "        \n",
    "        if self.imputer_type == \"knn\":\n",
    "            self.imputer = impute.KNNImputer()\n",
    "            X = self.imputer.fit(X)\n",
    "        \n",
    "        elif self.imputer_type == \"iterative\":\n",
    "\n",
    "            self.imputer = impute.IterativeImputer(max_iter=20, random_state=42, initial_strategy=\"most_frequent\", \n",
    "                                                  min_value=X.min(), max_value=X.max())\n",
    "\n",
    "\n",
    "            try:\n",
    "                X = self.imputer.fit(X)\n",
    "            except (ValueError, np.linalg.LinAlgError):\n",
    "                print(\"Jeden error bol trapnuty, kedy funkcii vadili NaNs. Tento error je ale divny, lebo mu to vadi\", \\\n",
    "                  \"len prvy krat, a potom to uz ide...\")\n",
    "                X = self.imputer.fit(X)\n",
    "            \n",
    "        return self\n",
    "               \n",
    "\n",
    "    def transform(self, X):\n",
    "  \n",
    "        X = X.copy()\n",
    "        \n",
    "        indices = X.index\n",
    "        columns = X.columns\n",
    "    \n",
    "        X = self.ordinal_encoder.transform(X)\n",
    "        X = self.imputer.transform(X).round()\n",
    "        \n",
    "        X = pd.DataFrame(data=X, columns=columns, index=indices)\n",
    "        \n",
    "        X = self.ordinal_encoder.inverse_transform(X)\n",
    "        \n",
    "        return X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ColumnTransformer je sice fajn, ze dokaze konkretne transformacie aplikovat na nami vybrane atributy, no vysledkom daneho \n",
    "#ColumnTransformer triedy je numpy array, nie dataframe, co je zle, pokial chceme napriklad viackrat pouzivat \n",
    "#ColumnTransformer a podobne.\n",
    "\n",
    "#Takze tato trieda sluzi ako wrapper okolo ColumnTransformer transformacie, kedy si uchovavame strukturu dataframu, teda\n",
    "#index, ako aj mena stlpcov, a nasledne, potom, co sa vykona ColumnTransformer, dany output vlozime do dataframu\n",
    "class WrapColumnTransformer(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def __init__(self, column_transformer, keep_original_cols=True, custom_cols_names=None):\n",
    "        self.column_transformer = column_transformer\n",
    "        self.keep_original_cols = keep_original_cols\n",
    "        self.custom_cols_names = custom_cols_names\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.column_transformer.fit(X)\n",
    "        return self\n",
    "        \n",
    "            \n",
    "    def transform(self, X):\n",
    "        indices = X.index\n",
    "        \n",
    "        columns = []\n",
    "        \n",
    "        for transf in self.column_transformer.transformers:\n",
    "            columns += transf[2]\n",
    "           \n",
    "\n",
    "        X = X.copy()\n",
    "        \n",
    "        X = self.column_transformer.transform(X)\n",
    "\n",
    "        if self.keep_original_cols == True:\n",
    "            X = pd.DataFrame(X, columns=columns, index=indices)\n",
    "        \n",
    "        elif self.custom_cols_names is not None:\n",
    "            X = pd.DataFrame(X, columns=self.custom_cols_names, index=indices)\n",
    "            \n",
    "        else:\n",
    "            X = pd.DataFrame(X, index=indices)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tu uz mame jednotlive skupiny atributov, pre ktore patria rozlicne sposoby aplikacie imputovania missing values.\n",
    "\n",
    "Konkretne pouzivame: \n",
    "- IterativeImputer pre medicinske data\n",
    "- Nas custom imputer, ktory pouziva KNNImputer pre atributy vztahov, prace, ci education (Dolezite je si vsimnut, ze sa nespracuvavaju spolu, ale oddelene to je zamerne)\n",
    "- SimpleImuter pre sex a age (znova je potrebne to na 2krat definovat, lebo jedno je spojity atribut, druhe kategoricky) \n",
    "\n",
    "Dovodom preco pouzivame imputer aj pri agu, i ked sme uz pouzili rok narodenia na imputaciu daneho atributu, je v pripade, kedy mame zaznam, kde je aj missing value, resp. zly value pre atribut age, ako aj je missing value pre atribut date_of_birth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "oxygen_attr = [\"mean_oxygen\", \"std_oxygen\", \"kurtosis_oxygen\", \"skewness_oxygen\"]\n",
    "glucose_attr = [\"mean_glucose\", \"std_glucose\", \"kurtosis_glucose\", \"skewness_glucose\"]\n",
    "\n",
    "vztahy_attr = [\"relationship\", \"marital-status\"]\n",
    "work_attr = [\"workclass\", \"occupation\", \"hours-per-week-cat\", \"income\"]\n",
    "edu_attr = [\"education\", \"education-num\"]\n",
    "\n",
    "impute_col_transf = compose.ColumnTransformer(transformers=[\n",
    "    (\"oxygen_n_glucose_impute\", KeepDataFrame(impute.IterativeImputer(max_iter=50)), oxygen_attr + glucose_attr),\n",
    "    (\"vztahy_impute\", CustomCatImputing(imputer_type=\"knn\"), vztahy_attr),\n",
    "    (\"work_impute\", CustomCatImputing(imputer_type=\"knn\"), work_attr),\n",
    "    (\"edu_impute\", CustomCatImputing(imputer_type=\"knn\"), edu_attr),\n",
    "    (\"sex_impute\", KeepDataFrame(impute.SimpleImputer(strategy=\"most_frequent\")), [\"sex\"]),\n",
    "    (\"age_impute\", KeepDataFrame(impute.SimpleImputer()), [\"age\"])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NonLinearTransf\n",
    "\n",
    "Tu aplikujeme non-linear transformacie na spojite atributy, podla toho, ktore transformacie najlepsie, v kombinacii s odstranovanim outlierov, zlepsili korelaciu danych atributov ku target atributu. Vyber transformacii bol vykonani v preprocessing notebooku, kde sme skusali jednotlive kombinacie non-linear transformacii s odstranovanim outlierov hladajuc optimalnu konfiguraciu..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sluzi na vratenie casti datasetu, ktory je outliermi\n",
    "def identify_outliers(a):\n",
    "    q25 = a.quantile(0.25)\n",
    "    q75 = a.quantile(0.75)\n",
    "    \n",
    "    iqr = q75-q25\n",
    "        \n",
    "    lower = q25 - 1.5 * iqr\n",
    "    upper = q75 + 1.5 * iqr\n",
    "    \n",
    "    return a[(a > upper) | (a < lower)]\n",
    "\n",
    "#odstranovanie outlierov pomocou takeho stylu, ze dany atribut, ktory riesime, rozdelime na \n",
    "#dvoje distribucie, podla target hodnoty, a jednotlivym distribuciach hladame outliery, ktore\n",
    "#nasledne odstranime\n",
    "def removing_outliers_per_class(data, column, clz=\"class\"):\n",
    "\n",
    "    data = data.copy()\n",
    "    \n",
    "    data_y0 = data[data[clz] == 0][column]\n",
    "    data_y1 = data[data[clz] == 1][column]\n",
    "        \n",
    "    idx = identify_outliers(data_y0).index.values\n",
    "    data = data.drop(index=idx)\n",
    "\n",
    "    idx = identify_outliers(data_y1).index.values\n",
    "    data = data.drop(index=idx)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ako mozno vidiet, tu mame zobrazene spojite atributy, ktorym boli aplikovane, resp. neaplikovane non-linear transforamacie, pricom sme pouzili:\n",
    "- PowerTransformer pre \"mean_oxygen\", \"skewness_oxygen\", \"kurtosis_oxygen\", \"skewness_glucose\"\n",
    "- QuantileTransformer pre \"age\"\n",
    "- Ziadnu transformaciu pre \"std_oxygen\", \"mean_glucose\", \"std_glucose\", \"kurtosis_glucose\" a samozrejme ostatne atributy, co su kategorickymi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_transf_attr = [\"mean_oxygen\", \"skewness_oxygen\", \"kurtosis_oxygen\", \"skewness_glucose\"]\n",
    "quant_transf_attr = [\"age\"]\n",
    "other_attr = [\"std_oxygen\", \"mean_glucose\", \"std_glucose\", \"kurtosis_glucose\", \"sex\", \"education\"] + vztahy_attr + work_attr\n",
    "\n",
    "non_linear_transf =  compose.ColumnTransformer(transformers=[\n",
    "   (\"power_transformer\", KeepDataFrame(preprocessing.PowerTransformer()), power_transf_attr),\n",
    "   (\"quantile_transformer\", KeepDataFrame(preprocessing.QuantileTransformer(output_distribution=\"normal\")), quant_transf_attr),\n",
    "   (\"pass\", \"passthrough\", other_attr)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Outliers - resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tu sa sustredime na odstranenie outlierov zo spojitych atributov, kedy toto je cast pipelinu, kedy sa pouziva tzv. resampling, a kvoli ktoremu sme museli pouzit specialny pipeline od kniznice imblearn - nejaka podnoz scikit-learnu.\n",
    "\n",
    "Pre resampling je typicke, ze sa vykonava len pre trenovaci dataset, pri testovacom sa nepouziva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlierRemoval(base.BaseEstimator):\n",
    "     \n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "        \n",
    "    def fit_resample(self, X, y):\n",
    "        return self.resample(X, y)\n",
    "                \n",
    "    def resample(self, X, y):\n",
    "        \n",
    "        X = X.copy()\n",
    "        y = y.copy()\n",
    "        \n",
    "        data = X.join(y, how=\"left\")\n",
    "        clz = \"class\"\n",
    "        \n",
    "        \n",
    "        for c in self.columns:\n",
    "            \n",
    "            data_y0 = data[data[clz] == 0][c]\n",
    "            data_y1 = data[data[clz] == 1][c]\n",
    "\n",
    "            idx = identify_outliers(data_y0).index.values\n",
    "            data = data.drop(index=idx)\n",
    "\n",
    "            idx = identify_outliers(data_y1).index.values\n",
    "            data = data.drop(index=idx)\n",
    "            \n",
    "        #toto je specialne pre target atribut\n",
    "        if data[clz].isnull().sum() > 0:\n",
    "            idx = data[data[clz].isnull()].index.values\n",
    "            data = data.drop(index=idx)\n",
    "\n",
    "            \n",
    "        X = data.drop(columns=[\"class\"])\n",
    "        y = data[\"class\"]\n",
    "            \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atributy, pre ktore riesime outliery su medicinske atributy (oxygen + glucose a age.\n",
    "\n",
    "Takiez vsak pri resamplovani sa berie do uvahy aj samotny target atribut, podla ktoreho sa taktiez odstranuju zaznamy v datasete - pokial target atribut je null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_columns = oxygen_attr + glucose_attr + [\"age\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling a Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posledna etapa pipelinu, kedy uz vsetky atributy su v spravnom tvare, odstranili sme vsetky mozne problemy - missing values, outliers a ine; a uz touto etapou len dalej transformujeme atributy, aby mohli by byt spracovane modelom/estimatorom.\n",
    "\n",
    "Teda v pripade numerickych atributov sa scaluje (vsade len StandardScaler) - lepsie vysledky pri trenovani modelu, \n",
    "\n",
    "a v pripade kategorickych atributov sa ciselne encoduju dane atributy, lebo vacsina modelov dokaze narabat len s ciselnymi atributami. Tu riesime otazku, ktory encoding je vhodny pre aky atribut, no drzali sme sa pravidla, ze nominalne atributy budu zaencodovane OneHotEncoder-om, zatial co ordinalne budu ce.OrdinalEncoder. Dovodom, preco sme si vybrali category_encoders Ordinal encoder a nie zo scikit learnu, je fakt, ze mi vieme urcit konkretny mapping hodnot. Bez toho neviem vobec, ci by sa pouzival ten OrdinalEncoder, kedze my chceme nim ukazat vztah medzi hodnotami daneho kategorickeho atributu, a pokial by mal zle namapovane hodnoty, tak potom dany vztah by sme dobre nezobrazili...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ako mozno vidiet, vybrali sme:\n",
    "- OrdinalEncoder pre education, income a hours-per-week - ordinalne atributy\n",
    "- OneHotEncoder pre ostatne kategoricke atributy - nominalne atributy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling = pipeline.Pipeline(steps=[\n",
    "    (\"standard_scaler\", preprocessing.StandardScaler())\n",
    "])\n",
    "\n",
    "onehot = pipeline.Pipeline(steps=[\n",
    "    (\"one_hot_enc\", preprocessing.OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "ord_mapping = [\n",
    "    {\"col\": \"education\", \"mapping\": {\n",
    "        \"Attending-school\": 1, \n",
    "        \"HS-grad\": 2,\n",
    "        \"Edu after HS, no uni\": 3,\n",
    "        \"Some-college\": 4,\n",
    "        \"Bachelors\": 5,\n",
    "        \"Masters/Doctorate\": 6}},\n",
    "    \n",
    "    {\"col\": \"hours-per-week-cat\", \"mapping\": {\n",
    "        \"<=35\": 1,\n",
    "        \"35< hours <=45\": 2,\n",
    "        \">45\": 3}},\n",
    "    \n",
    "    {\"col\": \"income\", \"mapping\": {\n",
    "        \"<=50K\": 1,\n",
    "        \">50K\": 2}}\n",
    "]\n",
    "\n",
    "\n",
    "ordinal = pipeline.Pipeline(steps=[\n",
    "    (\"ordinal_enc\", ce.OrdinalEncoder(mapping=ord_mapping, handle_unknown=\"return_nan\")),\n",
    "    (\"impute_unknown\", impute.SimpleImputer(strategy=\"most_frequent\"))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_attr = [\"age\"] + oxygen_attr + glucose_attr\n",
    "\n",
    "onehot_attr = [\"sex\", \"marital-status\", \"relationship\", \"occupation\", \"workclass\"]\n",
    "\n",
    "ordinal_attr = [\"education\", \"hours-per-week-cat\", \"income\"]\n",
    "\n",
    "last_col_transf = compose.ColumnTransformer(transformers=[\n",
    "    (\"num_attr_scaling\", scaling, scaling_attr),\n",
    "    (\"cat_attr_onehot_enc\", onehot, onehot_attr),\n",
    "    (\"cat_attr_ordinal_enc\", ordinal, ordinal_attr)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline ocakava, ze ako posledny krok dostane model, no my este nepracujeme so modelom, my chceme, aby nam ten pipeline vratil dataset, ktory presiel vsetkymi krokmi pipelinu, a tak vytvorime triedu, ktora sa bude hrat na model/estimator, no bude sluzit na vratenie datasetu, ktory prechadza cez dany pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tato trieda sa hra na klasifikator, aby mohla byt poslednym krokom v pipeline\n",
    "#sluzi na to, aby sme vedeli z pipelinu dostat nove X a y, ktore uz mozme rovno hodit do nejakeho modelu\n",
    "class Return_X_y(base.BaseEstimator, base.ClassifierMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def fit_predict(self, X, y=None):\n",
    "        self.fit(X,y)\n",
    "        return self.predict(X,y)\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        \n",
    "        if y is None:\n",
    "            return X.reset_index(drop=True)\n",
    "        \n",
    "        y = y.values\n",
    "        return X.reset_index(drop=True), y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vytvaram nazvy pre stlpce, ktore bude mat dataframe, ktory pipeline vracia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'mean_oxygen',\n",
       " 'std_oxygen',\n",
       " 'kurtosis_oxygen',\n",
       " 'skewness_oxygen',\n",
       " 'mean_glucose',\n",
       " 'std_glucose',\n",
       " 'kurtosis_glucose',\n",
       " 'skewness_glucose',\n",
       " 'sex_0',\n",
       " 'sex_1',\n",
       " 'marital-status_0',\n",
       " 'marital-status_1',\n",
       " 'marital-status_2',\n",
       " 'marital-status_3',\n",
       " 'relationship_0',\n",
       " 'relationship_1',\n",
       " 'relationship_2',\n",
       " 'relationship_3',\n",
       " 'occupation_0',\n",
       " 'occupation_1',\n",
       " 'occupation_2',\n",
       " 'occupation_3',\n",
       " 'occupation_4',\n",
       " 'occupation_5',\n",
       " 'occupation_6',\n",
       " 'occupation_7',\n",
       " 'occupation_8',\n",
       " 'workclass_0',\n",
       " 'workclass_1',\n",
       " 'education',\n",
       " 'hours-per-week-cat',\n",
       " 'income']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_cols_names = scaling_attr.copy()\n",
    "pocet_values = [2, 4, 4, 9, 2]\n",
    "\n",
    "for col, pocet in zip(onehot_attr, pocet_values):\n",
    "    for i in range(pocet):\n",
    "        custom_cols_names.append(col+\"_\"+str(i))\n",
    "    \n",
    "custom_cols_names += ordinal_attr\n",
    "custom_cols_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline v akcii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tu si mozeme skontrolovat, ci vsetko ide, ako ma. Pipeline pri trenovani (fit) vracia tuple (X,y), pri evaluacii (predict), vracia uz len X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-7f4a873f9dd0>:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mini_dataset.iloc[0][attr] = not_null.values[0]\n"
     ]
    }
   ],
   "source": [
    "X,y = one_proper_df(df1, df2, return_X_y=True)\n",
    "new_data = MAIN_PIPELINE.fit_predict(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3254, 33)\n",
      "(3254,)\n"
     ]
    }
   ],
   "source": [
    "#toto je shape noveho X a y\n",
    "print(new_data[0].shape)\n",
    "print(new_data[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tu mozno vidiet, ze ked uz netrenujeme model, tak nedochadza ku resamplingu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3933, 33)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAIN_PIPELINE.predict(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ulozenie datasetu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>mean_oxygen</th>\n",
       "      <th>std_oxygen</th>\n",
       "      <th>kurtosis_oxygen</th>\n",
       "      <th>skewness_oxygen</th>\n",
       "      <th>mean_glucose</th>\n",
       "      <th>std_glucose</th>\n",
       "      <th>kurtosis_glucose</th>\n",
       "      <th>skewness_glucose</th>\n",
       "      <th>sex_0</th>\n",
       "      <th>...</th>\n",
       "      <th>occupation_5</th>\n",
       "      <th>occupation_6</th>\n",
       "      <th>occupation_7</th>\n",
       "      <th>occupation_8</th>\n",
       "      <th>workclass_0</th>\n",
       "      <th>workclass_1</th>\n",
       "      <th>education</th>\n",
       "      <th>hours-per-week-cat</th>\n",
       "      <th>income</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.309275</td>\n",
       "      <td>-0.970829</td>\n",
       "      <td>-0.686117</td>\n",
       "      <td>0.764125</td>\n",
       "      <td>0.725318</td>\n",
       "      <td>0.605205</td>\n",
       "      <td>-0.380250</td>\n",
       "      <td>-0.589588</td>\n",
       "      <td>-0.351147</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.203812</td>\n",
       "      <td>-1.169635</td>\n",
       "      <td>-0.668252</td>\n",
       "      <td>1.157517</td>\n",
       "      <td>1.016598</td>\n",
       "      <td>0.579292</td>\n",
       "      <td>1.298640</td>\n",
       "      <td>-0.540657</td>\n",
       "      <td>-1.337825</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.809443</td>\n",
       "      <td>-1.084647</td>\n",
       "      <td>-0.899623</td>\n",
       "      <td>1.526660</td>\n",
       "      <td>1.718140</td>\n",
       "      <td>0.166750</td>\n",
       "      <td>-1.209121</td>\n",
       "      <td>-0.572961</td>\n",
       "      <td>0.588530</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.965891</td>\n",
       "      <td>-1.329568</td>\n",
       "      <td>-0.861695</td>\n",
       "      <td>1.599282</td>\n",
       "      <td>1.589144</td>\n",
       "      <td>1.046178</td>\n",
       "      <td>1.393176</td>\n",
       "      <td>-0.674814</td>\n",
       "      <td>-0.850382</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.440313</td>\n",
       "      <td>-0.159589</td>\n",
       "      <td>-0.176830</td>\n",
       "      <td>0.129948</td>\n",
       "      <td>0.034783</td>\n",
       "      <td>0.613355</td>\n",
       "      <td>1.094966</td>\n",
       "      <td>-0.525464</td>\n",
       "      <td>-0.844457</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3249</th>\n",
       "      <td>-1.500698</td>\n",
       "      <td>-0.725181</td>\n",
       "      <td>-0.737588</td>\n",
       "      <td>0.691051</td>\n",
       "      <td>0.810227</td>\n",
       "      <td>0.703377</td>\n",
       "      <td>0.751195</td>\n",
       "      <td>-0.634195</td>\n",
       "      <td>-0.597192</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3250</th>\n",
       "      <td>-0.026730</td>\n",
       "      <td>-0.247963</td>\n",
       "      <td>-0.509437</td>\n",
       "      <td>0.250342</td>\n",
       "      <td>0.276836</td>\n",
       "      <td>-0.019753</td>\n",
       "      <td>-1.224042</td>\n",
       "      <td>-0.343758</td>\n",
       "      <td>0.544180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3251</th>\n",
       "      <td>-0.515736</td>\n",
       "      <td>-0.985856</td>\n",
       "      <td>-0.720411</td>\n",
       "      <td>1.082239</td>\n",
       "      <td>1.054666</td>\n",
       "      <td>0.904298</td>\n",
       "      <td>-0.183360</td>\n",
       "      <td>-0.530779</td>\n",
       "      <td>-0.658326</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3252</th>\n",
       "      <td>0.957458</td>\n",
       "      <td>1.792960</td>\n",
       "      <td>2.798961</td>\n",
       "      <td>-1.711457</td>\n",
       "      <td>-1.918059</td>\n",
       "      <td>-1.482719</td>\n",
       "      <td>-1.471305</td>\n",
       "      <td>1.724890</td>\n",
       "      <td>1.645331</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3253</th>\n",
       "      <td>-0.670576</td>\n",
       "      <td>1.548362</td>\n",
       "      <td>2.243230</td>\n",
       "      <td>-1.363882</td>\n",
       "      <td>-1.462974</td>\n",
       "      <td>-0.553405</td>\n",
       "      <td>1.059238</td>\n",
       "      <td>0.261672</td>\n",
       "      <td>0.591205</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3254 rows √ó 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           age  mean_oxygen  std_oxygen  kurtosis_oxygen  skewness_oxygen  \\\n",
       "0     0.309275    -0.970829   -0.686117         0.764125         0.725318   \n",
       "1    -0.203812    -1.169635   -0.668252         1.157517         1.016598   \n",
       "2    -0.809443    -1.084647   -0.899623         1.526660         1.718140   \n",
       "3    -1.965891    -1.329568   -0.861695         1.599282         1.589144   \n",
       "4    -0.440313    -0.159589   -0.176830         0.129948         0.034783   \n",
       "...        ...          ...         ...              ...              ...   \n",
       "3249 -1.500698    -0.725181   -0.737588         0.691051         0.810227   \n",
       "3250 -0.026730    -0.247963   -0.509437         0.250342         0.276836   \n",
       "3251 -0.515736    -0.985856   -0.720411         1.082239         1.054666   \n",
       "3252  0.957458     1.792960    2.798961        -1.711457        -1.918059   \n",
       "3253 -0.670576     1.548362    2.243230        -1.363882        -1.462974   \n",
       "\n",
       "      mean_glucose  std_glucose  kurtosis_glucose  skewness_glucose  sex_0  \\\n",
       "0         0.605205    -0.380250         -0.589588         -0.351147    1.0   \n",
       "1         0.579292     1.298640         -0.540657         -1.337825    0.0   \n",
       "2         0.166750    -1.209121         -0.572961          0.588530    1.0   \n",
       "3         1.046178     1.393176         -0.674814         -0.850382    1.0   \n",
       "4         0.613355     1.094966         -0.525464         -0.844457    0.0   \n",
       "...            ...          ...               ...               ...    ...   \n",
       "3249      0.703377     0.751195         -0.634195         -0.597192    0.0   \n",
       "3250     -0.019753    -1.224042         -0.343758          0.544180    0.0   \n",
       "3251      0.904298    -0.183360         -0.530779         -0.658326    0.0   \n",
       "3252     -1.482719    -1.471305          1.724890          1.645331    1.0   \n",
       "3253     -0.553405     1.059238          0.261672          0.591205    0.0   \n",
       "\n",
       "      ...  occupation_5  occupation_6  occupation_7  occupation_8  \\\n",
       "0     ...           1.0           0.0           0.0           0.0   \n",
       "1     ...           0.0           0.0           1.0           0.0   \n",
       "2     ...           0.0           0.0           0.0           1.0   \n",
       "3     ...           0.0           0.0           1.0           0.0   \n",
       "4     ...           0.0           1.0           0.0           0.0   \n",
       "...   ...           ...           ...           ...           ...   \n",
       "3249  ...           0.0           0.0           1.0           0.0   \n",
       "3250  ...           0.0           0.0           0.0           0.0   \n",
       "3251  ...           0.0           0.0           0.0           0.0   \n",
       "3252  ...           0.0           0.0           1.0           0.0   \n",
       "3253  ...           0.0           0.0           0.0           1.0   \n",
       "\n",
       "      workclass_0  workclass_1  education  hours-per-week-cat  income  class  \n",
       "0             1.0          0.0        2.0                 2.0     1.0    0.0  \n",
       "1             1.0          0.0        4.0                 2.0     1.0    0.0  \n",
       "2             1.0          0.0        2.0                 2.0     1.0    0.0  \n",
       "3             0.0          1.0        2.0                 2.0     1.0    0.0  \n",
       "4             0.0          1.0        2.0                 3.0     2.0    0.0  \n",
       "...           ...          ...        ...                 ...     ...    ...  \n",
       "3249          0.0          1.0        2.0                 2.0     1.0    0.0  \n",
       "3250          0.0          1.0        5.0                 2.0     2.0    0.0  \n",
       "3251          0.0          1.0        5.0                 3.0     2.0    0.0  \n",
       "3252          0.0          1.0        2.0                 2.0     1.0    1.0  \n",
       "3253          0.0          1.0        2.0                 2.0     1.0    1.0  \n",
       "\n",
       "[3254 rows x 34 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame(new_data[0])\n",
    "y = pd.Series(new_data[1])\n",
    "\n",
    "data[\"class\"] = y\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                   0\n",
       "mean_oxygen           0\n",
       "std_oxygen            0\n",
       "kurtosis_oxygen       0\n",
       "skewness_oxygen       0\n",
       "mean_glucose          0\n",
       "std_glucose           0\n",
       "kurtosis_glucose      0\n",
       "skewness_glucose      0\n",
       "sex_0                 0\n",
       "sex_1                 0\n",
       "marital-status_0      0\n",
       "marital-status_1      0\n",
       "marital-status_2      0\n",
       "marital-status_3      0\n",
       "relationship_0        0\n",
       "relationship_1        0\n",
       "relationship_2        0\n",
       "relationship_3        0\n",
       "occupation_0          0\n",
       "occupation_1          0\n",
       "occupation_2          0\n",
       "occupation_3          0\n",
       "occupation_4          0\n",
       "occupation_5          0\n",
       "occupation_6          0\n",
       "occupation_7          0\n",
       "occupation_8          0\n",
       "workclass_0           0\n",
       "workclass_1           0\n",
       "education             0\n",
       "hours-per-week-cat    0\n",
       "income                0\n",
       "class                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"./preprocessed_data/train.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
